"""
–ö–≤–∞–Ω—Ç–æ–≤–∞—è –ø–∞–º—è—Ç—å mozgach108 - —Å–∏—Å—Ç–µ–º–∞ –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏
"""

import asyncio
import logging
import json
import hashlib
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class MemoryEntry:
    """–ó–∞–ø–∏—Å—å –≤ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏"""
    entry_id: str
    content: str
    context: str
    domain: str
    confidence: float
    access_count: int
    created_at: datetime
    last_accessed: datetime
    quantum_signature: str
    related_entries: List[str]


@dataclass
class MemoryPattern:
    """–ü–∞—Ç—Ç–µ—Ä–Ω –≤ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏"""
    pattern_id: str
    description: str
    frequency: int
    domains: List[str]
    keywords: List[str]
    strength: float


class QuantumMemory:
    """
    –°–∏—Å—Ç–µ–º–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è mozgach108
    
    –•—Ä–∞–Ω–∏—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–æ–ª–≥–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–º—è—Ç—å —Å–∏—Å—Ç–µ–º—ã,
    –≤–∫–ª—é—á–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–æ–≤, –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
    –∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è.
    """
    
    def __init__(self, memory_dir: str = "quantum_memory"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏
        
        Args:
            memory_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
        """
        self.memory_dir = Path(memory_dir)
        self.memory_dir.mkdir(exist_ok=True)
        
        self.entries: Dict[str, MemoryEntry] = {}
        self.patterns: Dict[str, MemoryPattern] = {}
        self.context_window = 100  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞
        self.max_entries = 10000   # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
        self.cleanup_threshold = 0.1  # –ü–æ—Ä–æ–≥ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∑–∞–ø–∏—Å–µ–π
        
        # –ö–≤–∞–Ω—Ç–æ–≤—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.domain_index: Dict[str, List[str]] = {}
        self.keyword_index: Dict[str, List[str]] = {}
        self.temporal_index: Dict[str, List[str]] = {}
        
        logger.info("üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ø–∞–º—è—Ç—å
            await self._load_memory()
            
            # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å—ã
            await self._build_indices()
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –ø–∞–º—è—Ç—å
            await self._optimize_memory()
            
            logger.info(f"‚úÖ –ö–≤–∞–Ω—Ç–æ–≤–∞—è –ø–∞–º—è—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ ({len(self.entries)} –∑–∞–ø–∏—Å–µ–π)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏: {e}")
            raise
    
    async def _load_memory(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤"""
        logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–∞–ø–∏—Å–∏ –ø–∞–º—è—Ç–∏
        entries_file = self.memory_dir / "entries.json"
        if entries_file.exists():
            try:
                with open(entries_file, 'r', encoding='utf-8') as f:
                    entries_data = json.load(f)
                
                for entry_data in entries_data:
                    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º datetime –æ–±—ä–µ–∫—Ç—ã
                    entry_data['created_at'] = datetime.fromisoformat(entry_data['created_at'])
                    entry_data['last_accessed'] = datetime.fromisoformat(entry_data['last_accessed'])
                    
                    entry = MemoryEntry(**entry_data)
                    self.entries[entry.entry_id] = entry
                
                logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.entries)} –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–ø–∏—Å–µ–π –ø–∞–º—è—Ç–∏: {e}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        patterns_file = self.memory_dir / "patterns.json"
        if patterns_file.exists():
            try:
                with open(patterns_file, 'r', encoding='utf-8') as f:
                    patterns_data = json.load(f)
                
                for pattern_data in patterns_data:
                    pattern = MemoryPattern(**pattern_data)
                    self.patterns[pattern.pattern_id] = pattern
                
                logger.info(f"üì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")
    
    async def _build_indices(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        logger.debug("üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
        
        self.domain_index.clear()
        self.keyword_index.clear()
        self.temporal_index.clear()
        
        for entry_id, entry in self.entries.items():
            # –ò–Ω–¥–µ–∫—Å –ø–æ –¥–æ–º–µ–Ω–∞–º
            if entry.domain not in self.domain_index:
                self.domain_index[entry.domain] = []
            self.domain_index[entry.domain].append(entry_id)
            
            # –ò–Ω–¥–µ–∫—Å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keywords = self._extract_keywords(entry.content + " " + entry.context)
            for keyword in keywords:
                if keyword not in self.keyword_index:
                    self.keyword_index[keyword] = []
                self.keyword_index[keyword].append(entry_id)
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å (–ø–æ –¥–Ω—è–º)
            date_key = entry.created_at.date().isoformat()
            if date_key not in self.temporal_index:
                self.temporal_index[date_key] = []
            self.temporal_index[date_key].append(entry_id)
        
        logger.debug("‚úÖ –ò–Ω–¥–µ–∫—Å—ã –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã")
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        import re
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words = re.findall(r'\b\w{3,}\b', text.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            '—ç—Ç–æ', '–∫–∞–∫', '—á—Ç–æ', '–¥–ª—è', '–∏–ª–∏', '–µ—Å–ª–∏', '—Ç–∞–∫', '–≤—Å–µ', '–æ–Ω–∏',
            '–æ–Ω–∞', '–µ–≥–æ', '–µ—ë', '—É–∂–µ', '–µ—â–µ', '–ø—Ä–∏', '–ø—Ä–æ', '—Ç–æ–º', '—Ç–æ—Ç',
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can'
        }
        
        keywords = [word for word in words if word not in stop_words and len(word) > 3]
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        return list(set(keywords))[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    async def store_memory(self, content: str, context: str, domain: str, confidence: float = 0.8) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –∫–≤–∞–Ω—Ç–æ–≤—É—é –ø–∞–º—è—Ç—å
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Å–∏
            domain: –î–æ–º–µ–Ω –∑–Ω–∞–Ω–∏–π
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∑–∞–ø–∏—Å–∏
            
        Returns:
            ID —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∑–∞–ø–∏—Å–∏
        """
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
        entry_id = self._generate_entry_id(content, context)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è –∑–∞–ø–∏—Å—å
        if entry_id in self.entries:
            existing_entry = self.entries[entry_id]
            existing_entry.access_count += 1
            existing_entry.last_accessed = datetime.now()
            existing_entry.confidence = max(existing_entry.confidence, confidence)
            
            logger.debug(f"üìù –û–±–Ω–æ–≤–ª–µ–Ω–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∑–∞–ø–∏—Å—å: {entry_id}")
            return entry_id
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
        quantum_signature = self._generate_quantum_signature(content, domain)
        related_entries = await self._find_related_entries(content, domain)
        
        new_entry = MemoryEntry(
            entry_id=entry_id,
            content=content,
            context=context,
            domain=domain,
            confidence=confidence,
            access_count=1,
            created_at=datetime.now(),
            last_accessed=datetime.now(),
            quantum_signature=quantum_signature,
            related_entries=related_entries
        )
        
        self.entries[entry_id] = new_entry
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å—ã
        await self._update_indices(entry_id, new_entry)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç—ã –ø–∞–º—è—Ç–∏
        if len(self.entries) > self.max_entries:
            await self._cleanup_memory()
        
        logger.debug(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–æ–≤–∞—è –∑–∞–ø–∏—Å—å: {entry_id}")
        return entry_id
    
    def _generate_entry_id(self, content: str, context: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è ID –∑–∞–ø–∏—Å–∏"""
        combined = f"{content}:{context}"
        return hashlib.sha256(combined.encode()).hexdigest()[:16]
    
    def _generate_quantum_signature(self, content: str, domain: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–æ–¥–ø–∏—Å–∏ –∑–∞–ø–∏—Å–∏"""
        signature_data = f"{content}:{domain}:quantum_memory"
        return hashlib.sha256(signature_data.encode()).hexdigest()[:12]
    
    async def _find_related_entries(self, content: str, domain: str, limit: int = 5) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π"""
        related = []
        content_keywords = self._extract_keywords(content)
        
        # –ò—â–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ —Ç–æ–º –∂–µ –¥–æ–º–µ–Ω–µ
        domain_entries = self.domain_index.get(domain, [])
        
        scores = {}
        for entry_id in domain_entries:
            if entry_id not in self.entries:
                continue
                
            entry = self.entries[entry_id]
            entry_keywords = self._extract_keywords(entry.content)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            common_keywords = set(content_keywords).intersection(set(entry_keywords))
            if common_keywords:
                similarity = len(common_keywords) / max(len(content_keywords), len(entry_keywords))
                scores[entry_id] = similarity
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-N –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø–∏—Å–µ–π
        sorted_related = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        related = [entry_id for entry_id, _ in sorted_related[:limit]]
        
        return related
    
    async def _update_indices(self, entry_id: str, entry: MemoryEntry):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –Ω–æ–≤–æ–π –∑–∞–ø–∏—Å–∏"""
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –¥–æ–º–µ–Ω–æ–≤
        if entry.domain not in self.domain_index:
            self.domain_index[entry.domain] = []
        self.domain_index[entry.domain].append(entry_id)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–µ–∫—Å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self._extract_keywords(entry.content + " " + entry.context)
        for keyword in keywords:
            if keyword not in self.keyword_index:
                self.keyword_index[keyword] = []
            self.keyword_index[keyword].append(entry_id)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω–¥–µ–∫—Å
        date_key = entry.created_at.date().isoformat()
        if date_key not in self.temporal_index:
            self.temporal_index[date_key] = []
        self.temporal_index[date_key].append(entry_id)
    
    async def retrieve_memories(self, query: str, domain: Optional[str] = None, 
                              limit: int = 10) -> List[MemoryEntry]:
        """
        –ü–æ–∏—Å–∫ –∑–∞–ø–∏—Å–µ–π –≤ –ø–∞–º—è—Ç–∏
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            domain: –§–∏–ª—å—Ç—Ä –ø–æ –¥–æ–º–µ–Ω—É
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
        """
        query_keywords = self._extract_keywords(query)
        candidate_entries = set()
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword in query_keywords:
            if keyword in self.keyword_index:
                candidate_entries.update(self.keyword_index[keyword])
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–æ–º–µ–Ω—É
        if domain and domain in self.domain_index:
            domain_entries = set(self.domain_index[domain])
            candidate_entries = candidate_entries.intersection(domain_entries)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        scored_entries = []
        for entry_id in candidate_entries:
            if entry_id not in self.entries:
                continue
                
            entry = self.entries[entry_id]
            score = self._calculate_relevance_score(entry, query_keywords)
            
            if score > 0:
                scored_entries.append((entry, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —á–∞—Å—Ç–æ—Ç–µ –æ–±—Ä–∞—â–µ–Ω–∏–π
        scored_entries.sort(key=lambda x: (x[1], x[0].access_count), reverse=True)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–æ—Å—Ç—É–ø–∞
        result_entries = []
        for entry, _ in scored_entries[:limit]:
            entry.access_count += 1
            entry.last_accessed = datetime.now()
            result_entries.append(entry)
        
        logger.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(result_entries)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}")
        return result_entries
    
    def _calculate_relevance_score(self, entry: MemoryEntry, query_keywords: List[str]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∑–∞–ø–∏—Å–∏ –∫ –∑–∞–ø—Ä–æ—Å—É"""
        entry_keywords = self._extract_keywords(entry.content + " " + entry.context)
        
        if not entry_keywords:
            return 0.0
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–∞—é—â–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        common_keywords = set(query_keywords).intersection(set(entry_keywords))
        keyword_score = len(common_keywords) / len(query_keywords) if query_keywords else 0
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏
        confidence_score = entry.confidence
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –æ–±—Ä–∞—â–µ–Ω–∏–π (–ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å)
        access_score = min(entry.access_count / 10.0, 1.0)
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º —Å–≤–µ–∂–µ—Å—Ç—å –∑–∞–ø–∏—Å–∏
        days_old = (datetime.now() - entry.created_at).days
        recency_score = max(0, 1.0 - days_old / 365.0)  # –°–Ω–∏–∂–µ–Ω–∏–µ –∑–∞ –≥–æ–¥ –¥–æ 0
        
        # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        total_score = (
            0.4 * keyword_score +
            0.3 * confidence_score +
            0.2 * access_score +
            0.1 * recency_score
        )
        
        return total_score
    
    async def enhance_response(self, response_data: Dict[str, Any], query: str) -> Dict[str, Any]:
        """
        –û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤—É—é –ø–∞–º—è—Ç—å
        
        Args:
            response_data: –ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞
            query: –ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞
        """
        logger.debug("üß† –û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤—É—é –ø–∞–º—è—Ç—å...")
        
        try:
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
            relevant_memories = await self.retrieve_memories(query, limit=3)
            
            enhanced_content = response_data.get("content", "")
            confidence = response_data.get("confidence", 0.5)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –ø–∞–º—è—Ç–∏
            if relevant_memories:
                memory_context = "\n\nüß† –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏:"
                
                for memory in relevant_memories:
                    if memory.confidence > 0.7:  # –¢–æ–ª—å–∫–æ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
                        memory_context += f"\n‚Ä¢ {memory.content[:100]}..."
                
                enhanced_content += memory_context
                
                # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è
                memory_boost = min(0.2, len(relevant_memories) * 0.05)
                confidence = min(1.0, confidence + memory_boost)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç—Ç–æ—Ç –¥–∏–∞–ª–æ–≥ –≤ –ø–∞–º—è—Ç—å
            await self.store_memory(
                content=enhanced_content,
                context=query,
                domain=response_data.get("quantum_state", {}).get("primary_domain", "general"),
                confidence=confidence
            )
            
            return {
                "content": enhanced_content,
                "confidence": confidence,
                "memory_enhanced": len(relevant_memories) > 0,
                "memory_context_count": len(relevant_memories)
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")
            return response_data
    
    async def analyze_patterns(self) -> List[MemoryPattern]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –ø–∞–º—è—Ç–∏"""
        logger.info("üîç –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∞—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keyword_frequencies = {}
        domain_distributions = {}
        
        for entry in self.entries.values():
            # –ü–æ–¥—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keywords = self._extract_keywords(entry.content + " " + entry.context)
            for keyword in keywords:
                keyword_frequencies[keyword] = keyword_frequencies.get(keyword, 0) + 1
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–æ–º–µ–Ω–∞–º
            domain_distributions[entry.domain] = domain_distributions.get(entry.domain, 0) + 1
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —á–∞—Å—Ç—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        new_patterns = []
        
        # –¢–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        top_keywords = sorted(keyword_frequencies.items(), key=lambda x: x[1], reverse=True)[:20]
        
        for i, (keyword, frequency) in enumerate(top_keywords):
            if frequency > 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞
                pattern_id = f"keyword_pattern_{keyword}_{i}"
                
                # –ù–∞—Ö–æ–¥–∏–º –¥–æ–º–µ–Ω—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–∏–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
                related_domains = []
                for entry in self.entries.values():
                    entry_keywords = self._extract_keywords(entry.content + " " + entry.context)
                    if keyword in entry_keywords:
                        if entry.domain not in related_domains:
                            related_domains.append(entry.domain)
                
                pattern = MemoryPattern(
                    pattern_id=pattern_id,
                    description=f"–ß–∞—Å—Ç–æ–µ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ: {keyword}",
                    frequency=frequency,
                    domains=related_domains,
                    keywords=[keyword],
                    strength=min(1.0, frequency / 50.0)
                )
                
                new_patterns.append(pattern)
                self.patterns[pattern_id] = pattern
        
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(new_patterns)} –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        return new_patterns
    
    async def _cleanup_memory(self):
        """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π –ø–∞–º—è—Ç–∏"""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
        
        current_time = datetime.now()
        entries_to_remove = []
        
        for entry_id, entry in self.entries.items():
            # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏ —Å –Ω–∏–∑–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é
            days_since_access = (current_time - entry.last_accessed).days
            
            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è:
            # - –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏ —Ä–µ–¥–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
            # - –°—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ —Å –æ—á–µ–Ω—å —Ä–µ–¥–∫–∏–º –¥–æ—Å—Ç—É–ø–æ–º
            should_remove = (
                (entry.confidence < self.cleanup_threshold and entry.access_count < 3) or
                (days_since_access > 365 and entry.access_count < 2) or
                (entry.confidence < 0.3 and days_since_access > 180)
            )
            
            if should_remove:
                entries_to_remove.append(entry_id)
        
        # –£–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å–∏
        removed_count = 0
        for entry_id in entries_to_remove:
            if len(self.entries) <= self.max_entries * 0.8:  # –û—Å—Ç–∞–≤–ª—è–µ–º 80% –∑–∞–ø–∏—Å–µ–π
                break
                
            del self.entries[entry_id]
            removed_count += 1
        
        # –ü–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        if removed_count > 0:
            await self._build_indices()
        
        logger.info(f"üßπ –£–¥–∞–ª–µ–Ω–æ {removed_count} –∑–∞–ø–∏—Å–µ–π –∏–∑ –ø–∞–º—è—Ç–∏")
    
    async def _optimize_memory(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏"""
        logger.debug("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        await self.analyze_patterns()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–≤—è–∑–∏ –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏
        await self._update_related_entries()
        
        logger.debug("‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    async def _update_related_entries(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏"""
        logger.debug("üîó –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∑–∞–ø–∏—Å—è–º–∏...")
        
        for entry_id, entry in self.entries.items():
            # –ù–∞—Ö–æ–¥–∏–º –Ω–æ–≤—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            new_related = await self._find_related_entries(entry.content, entry.domain)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
            entry.related_entries = new_related
    
    async def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∫–≤–∞–Ω—Ç–æ–≤–æ–π –ø–∞–º—è—Ç–∏...")
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø–∏—Å–∏
            entries_data = []
            for entry in self.entries.values():
                entry_dict = asdict(entry)
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º datetime –≤ —Å—Ç—Ä–æ–∫—É
                entry_dict['created_at'] = entry.created_at.isoformat()
                entry_dict['last_accessed'] = entry.last_accessed.isoformat()
                entries_data.append(entry_dict)
            
            entries_file = self.memory_dir / "entries.json"
            with open(entries_file, 'w', encoding='utf-8') as f:
                json.dump(entries_data, f, ensure_ascii=False, indent=2)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            patterns_data = [asdict(pattern) for pattern in self.patterns.values()]
            patterns_file = self.memory_dir / "patterns.json"
            with open(patterns_file, 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(self.entries)} –∑–∞–ø–∏—Å–µ–π –∏ {len(self.patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–∞–º—è—Ç–∏: {e}")
            raise
    
    def get_memory_usage(self) -> int:
        """–ü–æ–ª—É—á–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ MB"""
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
        entries_size = len(self.entries) * 2  # ~2KB –Ω–∞ –∑–∞–ø–∏—Å—å
        patterns_size = len(self.patterns) * 1  # ~1KB –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω
        indices_size = sum(len(index) for index in [
            self.domain_index, self.keyword_index, self.temporal_index
        ]) * 0.1  # ~100B –Ω–∞ –∏–Ω–¥–µ–∫—Å
        
        total_size_kb = entries_size + patterns_size + indices_size
        return int(total_size_kb / 1024)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ MB
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞–º—è—Ç–∏"""
        if not self.entries:
            return {
                "total_entries": 0,
                "total_patterns": 0,
                "memory_usage_mb": 0,
                "domains": {},
                "average_confidence": 0.0
            }
        
        domain_counts = {}
        total_confidence = 0.0
        total_access_count = 0
        
        for entry in self.entries.values():
            domain_counts[entry.domain] = domain_counts.get(entry.domain, 0) + 1
            total_confidence += entry.confidence
            total_access_count += entry.access_count
        
        return {
            "total_entries": len(self.entries),
            "total_patterns": len(self.patterns),
            "memory_usage_mb": self.get_memory_usage(),
            "domains": domain_counts,
            "average_confidence": total_confidence / len(self.entries),
            "total_access_count": total_access_count,
            "index_sizes": {
                "domain_index": len(self.domain_index),
                "keyword_index": len(self.keyword_index),
                "temporal_index": len(self.temporal_index)
            }
        }
